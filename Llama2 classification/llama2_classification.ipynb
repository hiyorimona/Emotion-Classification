{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJvidoTew1ML"
   },
   "source": [
    "# Prepocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:18.346202200Z",
     "start_time": "2024-04-02T17:09:17.257074100Z"
    },
    "id": "xnH9pmq_jO1l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook as tq\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:18.362176100Z",
     "start_time": "2024-04-02T17:09:18.351638600Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:19.192609700Z",
     "start_time": "2024-04-02T17:09:18.362176100Z"
    },
    "id": "IPKEHEYKVt9p"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/emotion_detection_group_old.csv\").loc[lambda x: x[\"emotion\"] != \"neutral\"]\n",
    "df_test = pd.read_csv(\"data/all_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"data/emotion_detection_group_old_comp.csv\", compression=\"zip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:19.208389300Z",
     "start_time": "2024-04-02T17:09:19.192609700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((547028, 2), (1436, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:19.271338300Z",
     "start_time": "2024-04-02T17:09:19.208389300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547028, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "disgust        5550\n",
       "surprise      35989\n",
       "fear          59695\n",
       "anger         77918\n",
       "sadness      143454\n",
       "happiness    224422\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train = pd.concat([df_disgust, df_train])\n",
    "print(df_train.shape)\n",
    "df_train['emotion'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_multiple_classes(final_df, emotion_column_name):\n",
    "    # Get the minimum count of samples among all classes\n",
    "    min_count = final_df[emotion_column_name].value_counts().min()\n",
    "    \n",
    "    # Create an empty DataFrame to store the balanced data\n",
    "    balanced_df = pd.DataFrame(columns=final_df.columns)\n",
    "    \n",
    "    # Iterate over unique emotions\n",
    "    for emotion in final_df[emotion_column_name].unique():\n",
    "        emotion_df = final_df[final_df[emotion_column_name] == emotion]        \n",
    "        if len(emotion_df) > min_count:\n",
    "            emotion_df = emotion_df.sample(n=min_count, random_state=1)\n",
    "        \n",
    "        balanced_df = pd.concat([balanced_df, emotion_df])\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "happiness    5550\n",
       "sadness      5550\n",
       "anger        5550\n",
       "fear         5550\n",
       "surprise     5550\n",
       "disgust      5550\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced = balancing_multiple_classes(df_train,'emotion')\n",
    "df_balanced['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:23.351501Z",
     "start_time": "2024-04-02T17:09:23.276320100Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion_dummies_train = pd.get_dummies(df_train['emotion'])\n",
    "emotion_dummies_train = emotion_dummies_train.astype(int)\n",
    "\n",
    "emotion_dummies_test = pd.get_dummies(df_test['emotion'])\n",
    "emotion_dummies_test = emotion_dummies_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:23.863483500Z",
     "start_time": "2024-04-02T17:09:23.823740900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "ap6Fv9HLxRBm",
    "outputId": "1a5b18af-7d36-4691-a3cf-33ef5321e9f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happiness</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Girls are happy when they get flowers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His jaw dropped in disbelief when he saw the price of the concert tickets.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sometimes the ugly stench makes me wanna throw up.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The foul odor from the garbage bin was disgusting.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     sentence  \\\n",
       "0                                       Girls are happy when they get flowers   \n",
       "1  His jaw dropped in disbelief when he saw the price of the concert tickets.   \n",
       "2                          Sometimes the ugly stench makes me wanna throw up.   \n",
       "3                          The foul odor from the garbage bin was disgusting.   \n",
       "\n",
       "   anger  disgust  fear  happiness  sadness  surprise  \n",
       "0      0        0     0          1        0         0  \n",
       "1      0        0     0          0        0         1  \n",
       "2      0        1     0          0        0         0  \n",
       "3      0        1     0          0        0         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_main = pd.concat([df_train['sentence'], emotion_dummies_train], axis=1)\n",
    "df_test_main = pd.concat([df_test['sentence'], emotion_dummies_test], axis=1)\n",
    "\n",
    "df_test_main.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:24.587605500Z",
     "start_time": "2024-04-02T17:09:24.514095Z"
    },
    "id": "CouOCg2d_H1S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disgust      0.015\n",
       "fear         0.116\n",
       "happiness    0.394\n",
       "sadness      0.265\n",
       "surprise     0.063\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_train_main.sample(n = 1000, random_state = 4)\n",
    "columns = sample.columns\n",
    "freq = sample[columns[2:]].sum()/sample.shape[0]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:25.621452100Z",
     "start_time": "2024-04-02T17:09:25.596677700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "lWpsDK6cXAXQ",
    "outputId": "988475c6-f490-49e6-c51a-df60f4e0641a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happiness</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528526</th>\n",
       "      <td>Well . . . perhaps you should think about it .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126699</th>\n",
       "      <td>Haha if only he had reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324090</th>\n",
       "      <td>i feel good and am gaining weight much better than i did with diego</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482598</th>\n",
       "      <td>i am feeling a bit unsure of myself as i document the beautiful birth of my fourth child</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502760</th>\n",
       "      <td>i feel so disgusted as i type this</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        sentence  \\\n",
       "528526                                            Well . . . perhaps you should think about it .   \n",
       "126699                                                                Haha if only he had reddit   \n",
       "324090                       i feel good and am gaining weight much better than i did with diego   \n",
       "482598  i am feeling a bit unsure of myself as i document the beautiful birth of my fourth child   \n",
       "502760                                                        i feel so disgusted as i type this   \n",
       "\n",
       "        anger  disgust  fear  happiness  sadness  surprise  \n",
       "528526      1        0     0          0        0         0  \n",
       "126699      0        0     0          1        0         0  \n",
       "324090      0        0     0          1        0         0  \n",
       "482598      0        0     1          0        0         0  \n",
       "502760      1        0     0          0        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama Multi-class text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:38.400694300Z",
     "start_time": "2024-04-02T17:09:30.283301400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from scripts.llama_tokenizer import TokenizedDataset\n",
    "from scripts.llama_model import LLamaClass\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:40.909493200Z",
     "start_time": "2024-04-02T17:09:40.882821600Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:43.823565100Z",
     "start_time": "2024-04-02T17:09:43.725142800Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_train_main = sample\n",
    "#df_train_main = df_train_main.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "training_df, validation_df = train_test_split(df_train_main, random_state=77, test_size=0.30, shuffle=True)\n",
    "testing_df = df_test_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:45.513575100Z",
     "start_time": "2024-04-02T17:09:45.493637500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (382919, 7), Val: (164109, 7), Test: (1436, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {training_df.shape}, Val: {validation_df.shape}, Test: {testing_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:46.782349900Z",
     "start_time": "2024-04-02T17:09:46.754076700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:52.790071300Z",
     "start_time": "2024-04-02T17:09:50.611536100Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "tokenizer.pad_token_id = tokenizer.vocab_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:55.231790500Z",
     "start_time": "2024-04-02T17:09:55.171580200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence: \n",
      "That depends do you have legs ?\n",
      "\n",
      "tokenized sentence: \n",
      "{'input_ids': tensor([[    1,  2193,  7111,   437,   366,   505, 21152,  1577, 31999, 31999,\n",
      "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
      "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
      "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
      "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "test_text = training_df['sentence'].sample().iloc[0]\n",
    "\n",
    "encodings = tokenizer.encode_plus(test_text, \n",
    "                                  add_special_tokens = True,\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\", \n",
    "                                  return_attention_mask = True, \n",
    "                                  return_tensors = \"pt\")\n",
    "\n",
    "print(f\"original sentence: \\n{test_text}\\n\")\n",
    "print(f\"tokenized sentence: \\n{encodings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:10:00.103400500Z",
     "start_time": "2024-04-02T17:10:00.074038400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_list = emotion_dummies_train.columns.tolist()\n",
    "emotion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:10:01.414874100Z",
     "start_time": "2024-04-02T17:10:01.367587700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedDataset(training_df, tokenizer, MAX_LEN, emotion_list)\n",
    "val_dataset = TokenizedDataset(validation_df, tokenizer, MAX_LEN, emotion_list)\n",
    "test_dataset = TokenizedDataset(testing_df, tokenizer, MAX_LEN, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:10:02.238503Z",
     "start_time": "2024-04-02T17:10:02.181860300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1, 23992,   526,  9796,   746,   896,   679, 18281, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999, 31999,\n",
       "         31999, 31999, 31999, 31999, 31999, 31999]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'emotion': tensor([0., 0., 0., 1., 0., 0.]),\n",
       " 'sentence': 'Girls are happy when they get flowers'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:10:04.378968600Z",
     "start_time": "2024-04-02T17:10:04.331244200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:10:32.741758900Z",
     "start_time": "2024-04-02T17:10:07.962163600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609d3401c0824ae6b9193f93131c5e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LLamaClass(\n",
       "  (llama): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LLamaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:11:58.100002500Z",
     "start_time": "2024-04-02T17:11:58.073243800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:11:59.176138900Z",
     "start_time": "2024-04-02T17:11:59.155869900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adam Optimizer \n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8, no_deprecation_warning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:11:59.843687100Z",
     "start_time": "2024-04-02T17:11:59.795406900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training of the model for one epoch\n",
    "def train_model(training_loader, model, optimizer):\n",
    "    print(\"Training:\")\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    \n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        emotions = data['emotion'].to(device, dtype = torch.float)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids) \n",
    "        loss = loss_fn(outputs, emotions)\n",
    "        losses.append(loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # training accuracy\n",
    "        _, preds = torch.max(outputs, dim=1) # batch dim \n",
    "        _, emotion = torch.max(emotions, dim=1)  # batch dim\n",
    "        num_samples += len(emotion)  \n",
    "        correct_predictions += torch.sum(preds == emotion)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:12:00.906004Z",
     "start_time": "2024-04-02T17:12:00.896955Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, optimizer):\n",
    "    print(\"Evaluating:\")\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    model.eval()\n",
    "    loop = tq.tqdm(enumerate(validation_loader), total=len(validation_loader), \n",
    "                      leave=True, colour='#8B0000')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in loop:\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            emotions = data['emotion'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "\n",
    "            loss = loss_fn(outputs, emotions)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            _, preds = torch.max(outputs, dim=1) \n",
    "            _, emotion = torch.max(emotions, dim=1) \n",
    "            num_samples += len(emotion) \n",
    "            correct_predictions += torch.sum(preds == emotion)\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llama.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 31999,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.39.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.llama.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T17:12:24.929172700Z",
     "start_time": "2024-04-02T17:12:20.073448600Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ad0868cf1e44f99b80fb1eb4b846b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 26.33 GiB is allocated by PyTorch, and 23.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     model, train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     val_acc, val_loss \u001b[38;5;241m=\u001b[39m eval_model(val_data_loader, model, optimizer)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(training_loader, model, optimizer)\u001b[0m\n\u001b[0;32m     18\u001b[0m emotions \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, emotions)\n\u001b[0;32m     23\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\Documents\\GitHub\\2023-24c-fai2-adsai-group-group5\\Llama2 classification\\scripts\\llama_model.py:12\u001b[0m, in \u001b[0;36mLLamaClass.forward\u001b[1;34m(self, input_ids, attn_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attn_mask, token_type_ids):\n\u001b[1;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# token_type_ids=token_type_ids\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     output_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output\u001b[38;5;241m.\u001b[39mpooler_output)\n\u001b[0;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(output_dropout)\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1007\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1013\u001b[0m         cache_position,\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:739\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 739\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:638\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    636\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m    637\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m--> 638\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    641\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 26.33 GiB is allocated by PyTorch, and 23.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    # save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        #torch.save(model.state_dict(), os.path.join(\"models\",\"roberta_model_3.pth\"))\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    \n",
    "    sentences = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    emotion_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for data in data_loader:\n",
    "        sentence = data[\"sentence\"]\n",
    "        ids = data[\"input_ids\"].to(device, dtype = torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        emotions = data[\"emotion\"].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        _, emotion = torch.max(emotions, dim=1)\n",
    "\n",
    "        sentences.extend(sentence)\n",
    "        predictions.extend(preds.tolist())                     # Convert tensor to list\n",
    "        prediction_probs.extend(outputs.tolist())              # Convert tensor to list\n",
    "        emotion_values.extend(emotions.argmax(dim=1).tolist())  # Convert tensor to list\n",
    "     \n",
    "    return sentences, predictions, prediction_probs, emotion_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, predictions, prediction_probs, emotion_values = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.86      0.75      0.80       253\n",
      "     disgust       0.76      0.90      0.82       199\n",
      "        fear       0.87      0.88      0.87       215\n",
      "   happiness       0.88      0.95      0.92       263\n",
      "     sadness       0.85      0.82      0.83       289\n",
      "    surprise       0.89      0.81      0.85       217\n",
      "\n",
      "    accuracy                           0.85      1436\n",
      "   macro avg       0.85      0.85      0.85      1436\n",
      "weighted avg       0.85      0.85      0.85      1436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(emotion_values, predictions, target_names=emotion_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_mapping = {'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'sadness': 4, 'surprise': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_emotions = [emotion for label in predictions for emotion, encoded_label in emotion_mapping.items() if label == encoded_label]\n",
    "len(predicted_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test set: 0.8501911273023104\n"
     ]
    }
   ],
   "source": [
    "# best so far 0.7588\n",
    "# best for 1 epoch unbalanced 0.83 \n",
    "print('F1 score on test set: {}'.format(f1_score(df_test['emotion'], predicted_emotions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'emotion': predicted_emotions,\n",
    "    'sentence' : df_test['sentence']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[['id', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m submission\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m==\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m submission\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m1436\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrobert_test.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmissions/roberta_test2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "if submission.columns.tolist() == ['id', 'emotion'] and submission.shape == (1436, 2):\n",
    "    torch.save(model.state_dict(), os.path.join(\"models\",\"robert_test.bin\"))\n",
    "    submission.to_csv(\"submissions/roberta_test2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a297a11173b4e5eb360ad40a27e12cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Ensure num_labels matches the number of unique labels\n",
    "unique_labels = df['emotion'].nunique()\n",
    "model = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\", num_labels=unique_labels)\n",
    "\n",
    "# Double-check the encoded labels\n",
    "print(\"Unique labels in the dataset:\", sorted(df['emotion'].unique()))\n",
    "\n",
    "# Verify that there are no unexpected values\n",
    "assert df['emotion'].max() < unique_labels, \"Found a label index higher than num_labels - 1\"\n",
    "assert df['emotion'].min() >= 0, \"Found a label index lower than 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch labels: 2\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(dataset))\n",
    "print(\"First batch labels:\", first_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEmotionDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encodings, labels):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodings \u001b[38;5;241m=\u001b[39m encodings\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming your dataset is already created\n",
    "dataset = EmotionDataset(encodings, df['emotion'].values)\n",
    "\n",
    "# Create a DataLoader to handle batching\n",
    "batch_size = 8  # You can adjust the batch size as needed\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now, iterate over the DataLoader, not the dataset\n",
    "for i, batch in enumerate(data_loader):\n",
    "    if i >= 5:  # Check the first 5 batches\n",
    "        break\n",
    "    print(f\"Batch {i} labels:\", batch['labels'])\n",
    "    # Now batch['labels'] should be a list/array/tensor\n",
    "    assert isinstance(batch['labels'], (list, np.ndarray, torch.Tensor)), \"Labels should be a list, numpy array, or torch Tensor.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tokenizer pad_token_id: 31999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45a2b2686cb490ea915cf233f9a5fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\training_args.py:1399: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of  Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73457ec4be749fdb41c8b7edc51c5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m\n\u001b[0;32m     67\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     68\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# the instantiated  Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,               \u001b[38;5;66;03m# training dataset\u001b[39;00m\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2124\u001b[0m ):\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1388\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1386\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1388\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1399\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1400\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:972\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    969\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m past_seen_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:  \u001b[38;5;66;03m# kept for BC (cache positions)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wesle\\anaconda3\\envs\\bert_nlp\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load your dataframe (adjust the path as needed)\n",
    "df = pd.read_csv(\"data/emotion_detection_group_old.csv\").loc[lambda x: x[\"emotion\"] != \"neutral\"]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotion'] = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenize the sentences\n",
    "encodings = tokenizer(df['sentence'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = EmotionDataset(encodings, df['emotion'].values)\n",
    "\n",
    "# Define the DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "if tokenizer.pad_token_id >= tokenizer.vocab_size:\n",
    "    new_pad_token_id = tokenizer.vocab_size - 1  # Last index in the vocabulary\n",
    "    tokenizer.pad_token_id = new_pad_token_id\n",
    "    print(\"New tokenizer pad_token_id:\", tokenizer.pad_token_id)\n",
    "\n",
    "# Load the Llama model with the correct number of labels\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset,               # training dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizer pad_token_id:\", tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc90e3c70f4bb4abae47033702d03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with input IDs: tensor([[    1,   474,   505,  ..., 32000, 32000, 32000],\n",
      "        [    1,   306, 13818,  ..., 32000, 32000, 32000],\n",
      "        [    1,   474,  4459,  ..., 32000, 32000, 32000],\n",
      "        ...,\n",
      "        [    1,   474,   626,  ..., 32000, 32000, 32000],\n",
      "        [    1,   474,  4459,  ..., 32000, 32000, 32000],\n",
      "        [    1, 15011,  1919,  ..., 32000, 32000, 32000]])\n",
      "Error with attention mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "Error with labels: tensor([0, 3, 4, 4, 3, 3, 3, 3])\n",
      "Index error during training, inputs printed above.\n"
     ]
    }
   ],
   "source": [
    "# Adjusted from the standard Trainer to add debugging information\n",
    "from transformers import Trainer\n",
    "\n",
    "class DebugTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        try:\n",
    "            return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
    "        except IndexError as e:\n",
    "            print(f\"Error with input IDs: {inputs['input_ids']}\")\n",
    "            print(f\"Error with attention mask: {inputs['attention_mask']}\")\n",
    "            print(f\"Error with labels: {inputs['labels']}\")\n",
    "            raise e\n",
    "\n",
    "# Initialize the DebugTrainer instead of the regular Trainer\n",
    "trainer = DebugTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Attempt to train\n",
    "try:\n",
    "    trainer.train()\n",
    "except IndexError:\n",
    "    print(\"Index error during training, inputs printed above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted tokenizer's pad_token_id: 31999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd0492ad2994f68877f14249c99c341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjust the tokenizer's pad_token_id\n",
    "tokenizer.pad_token_id = tokenizer.vocab_size - 1\n",
    "print(\"Adjusted tokenizer's pad_token_id:\", tokenizer.pad_token_id)\n",
    "\n",
    "# Re-tokenize the dataset\n",
    "encodings = tokenizer(df['sentence'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Ensure no token ID equals the previously problematic 32000\n",
    "assert all(32000 not in seq for seq in encodings['input_ids']), \"Old pad token ID is still present in the encodings.\"\n",
    "\n",
    "# Create the dataset and DataLoader again\n",
    "dataset = EmotionDataset(encodings, df['emotion'].values)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize the DebugTrainer with the new dataset\n",
    "trainer = DebugTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Attempt to train\n",
    "try:\n",
    "    trainer.train()\n",
    "except IndexError as e:\n",
    "    print(\"Index error during training, inputs printed above.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "data-preparation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
